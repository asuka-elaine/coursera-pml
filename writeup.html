Project Writeup for Course: Practical Machine Learning

Yiran Pang
 
Background
Human Activity Recognition(HAR) technology has emergied in the last few years. However, the focus has always been how much exercise a person did, instead of how well they do it. The purpose of this project is to develop a model to predict how well the exercise is done.
 
We build the model by leveraging the data recorded by the sensors experiment participants are wearing when doing weight lifting. We want to predict whether they are doing the exercise correctly, represented by the ¡®class¡¯ they belong (A-E).
 
The training and testing data sets are from http://groupware.les.inf.puc-rio.br/har
 
Detailed Steps
- Step 1: Data Loading
Load the datasets into R:
 
Code:
>library(caret)
>trainingOri <- read.csv("~/pml-training.csv", header=TRUE, na.strings = c("NA",""))
>testingOri <- read.csv("~/pml-testing.csv", header=TRUE, na.strings = c("NA",""))
 
There were 19622 observations with the training set. Each observation includes 159 predictors and 1 outcome value(classe).
 
- Step 2: Cleaning Raw dataset
I looked into the training dataset using following command:
>summary(trainingOri)
Several issues came out:
1)    A lot of predictors are having almost zero variances and hence shall be removed.(e.g. amplitude_yaw_belt)
 
Code:
#Remove variables with almost zero variances
>nzv<-nearZeroVar(trainingOri)
>trainingOri2 <- trainingOri[,-nzv]
>testingOri2 <-testingOri[,-nzv]
 
This reduced the predictor to 116.
 
2)    Several predictors have ¡®NA¡¯ value for 19216 out of 19622 predictions. They will mess up our final predictions and hence shall be removed as well.
 
Code:
>trainingOri3<-trainingOri2[,colSums(is.na(trainingOri2))==0]
>testingOri3<-testingOri2[,colSums(is.na(trainingOri2))==0]
 
This reduced the predictor to 58.
 
3)    There are some predictors that are either non-numeric or obviously won¡¯t contribute to the final prediction. In this step I removed four columns:
-       ¡®X¡¯: the row number indicator
-       ¡®user_name¡¯: since according to the data description each member is doing the exercise so that ¡®the execution complied to the manner they were supposed to simulate¡¯, this column shall not affect the outcome as well.
-       raw_timestamp_part_1: this is part 1 of the exercise timestamp and are in the level of 10^9. It isn¡¯t as informative as raw_timestamp_part_2 so I removed it as well.
-       cvtd_timestamp: the time information shall be sufficiently represented by raw_timestamp_part_2 and hence this column is removed as well in this step.
 
Code:
#Remove variables that are non-numeric or (apparently) non-informative
>trainingOriFin<-trainingOri3[,c(-1,-2,-3,-5)]
>testingOriFin<-testingOri3[,c(-1,-2,-3,-5)]
 
This reduced the predictor to 54.
 
- Step 3: Data Splitting and analysis
 
1)    Split the training set into 60% training£¨11776 observations£© and 40% testing(7846 observations):
 
Code:
#Data splitting
>inTrain<-createDataPartition(y=trainingOriFin$classe,p=0.6,list=FALSE)
>training <- trainingOriFin[inTrain,]
>testing<-trainingOriFin[-inTrain,]
>dim(training)
>dim(testing)
 
2)    I then looked into the covariance of the variables: a lot of variables are highly correlated:
 
Code:
>M<-abs(cor(training[,-55]))
>diag(M)<-0
>head(which(M>0.8,arr.ind=T))
 
Output:
                 row col
yaw_belt           5   3
total_accel_belt   6   3
accel_belt_y      11   3
accel_belt_z      12   3
accel_belt_x      10   4
magnet_belt_x     13   4
 
3)    Plot the highly correlated variables versus ¡®classe¡¯: these predictors are not only highly correlated with each other, they are also not informative in predicting:
 
Example: the indicator yaw_belt
 
Code:
>library(Hmisc)
>cutyb<-cut2(training$yaw_belt,g=5)
>qplot(c(1:11776),training$classe,color=cutyb)
 
Output:

 
 
- Step 4: Preprocessing, Model Building and checking
 
I then decide to take three approaches to build the model.
-       Approach 1: use original dataset as predictors
-       Approach 2: remove the highly correlated non-informative columns, use the rest as predictors
-       Approach 3: apply PCA to the original dataset to produce predictors
 
The conclusion is Approach 2 produced the best model considering accuracy and efficiency.
 
Approach 1: Predict using the training set without further preprocessing.
 
The accuracy for this is quite high(0.9977), yet it took forever to run hence isn¡¯t scalable.
 
Code:
>set.seed(726)
>modelFit <- train(training$classe ~.,data=training,method="rf",trControl = trainControl(method='cv'))
>confusionMatrix(testing$classe,predict(modelFit,testing))
 
Output:
 
Confusion Matrix and Statistics
 
          Reference
Prediction    A    B    C    D    E
         A 2232    0    0    0    0
         B    1 1515    2    0    0
         C    0    3 1365    0    0
         D    0    0    8 1278    0
         E    0    0    0    4 1438
 
Overall Statistics
                                         
               Accuracy : 0.9977         
                 95% CI : (0.9964, 0.9986)
    No Information Rate : 0.2846         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9971         
 Mcnemar's Test P-Value : NA             
 
 
Approach 2: Predict using the processed training set by directly excluding the highly correlated variables.
 
Accuracy is also really good(0.9973) and didn¡¯t take so much time.
 
Code:
>training2 <- training[,c(-3,-4,-5,-6,-10,-11,-12,-13,-20,-21,-24,-26,-27,-28,-30,-31,-33,-35,-36,-47,-48)]
>testing2 <- testing[,c(-3,-4,-5,-6,-10,-11,-12,-13,-20,-21,-24,-26,-27,-28,-30,-31,-33,-35,-36,-47,-48)]
>modelFit2 <- train(training$classe ~.,data=training2,method="rf",trControl = trainControl(method='cv'))
>confusionMatrix(testing$classe,predict(modelFit2,testing2))
Output:
 
Confusion Matrix and Statistics
 
          Reference
Prediction    A    B    C    D    E
         A 2231    0    0    0    1
         B    1 1514    3    0    0
         C    0    6 1362    0    0
         D    0    0    9 1277    0
         E    0    0    0    1 1441
 
Overall Statistics
                                         
               Accuracy : 0.9973         
                 95% CI : (0.9959, 0.9983)
    No Information Rate : 0.2845         
    P-Value [Acc > NIR] : < 2.2e-16      
                                          
                  Kappa : 0.9966         
 Mcnemar's Test P-Value : NA             
 
 
Approach 3: Predict using the processed training set by PCA(with threshold = 0.95).
 
Didn¡¯t take much time either. Yet the accuracy isn¡¯t as good(0.974).
 
Code:
>preProc <- preProcess(training[,-55],method="pca",thresh=0.95)
>trainPC <- predict(preProc,training[,-55])
>modelFit3 <- train(training$classe ~.,data=trainPC,method="rf",trControl = trainControl(method='cv'))
>testPC <- predict(preProc,testing[,-55])
>confusionMatrix(testing$classe,predict(modelFit3,testPC))
 
Output:
 
Confusion Matrix and Statistics
 
          Reference
Prediction    A    B    C    D    E
         A 2219    5    6    2    0
         B   20 1464   25    2    7
         C    3    7 1330   26    2
         D    0    2   78 1205    1
         E    0    4    4   10 1424
 
Overall Statistics
                                         
               Accuracy : 0.974          
                 95% CI : (0.9702, 0.9774)
    No Information Rate : 0.2858         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9671         
 Mcnemar's Test P-Value : NA             
 
- Step 5: Predict the final testing set.
I decided model2 is the best option. So I used it to predict the final testing set and get a 100% accuracy when submitting the output:
 
Code:
>testingOriFin2<-testingOriFin[,c(-3,-4,-5,-6,-10,-11,-12,-13,-20,-21,-24,-26,-27,-28,-30,-31,-33,-35,-36,-47,-48)]
>answers <- predict(modelFit2,testingOriFin2)
>answers
 
Output:
[1] B A B A A E D B A A B C B A E E A B B B
